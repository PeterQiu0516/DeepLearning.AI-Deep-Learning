# Course 2 - Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Notes and exercises for the second course of the Deep Learning Specialization offered by DeepLearning.AI

## Course Syllabus

+ ### Week 1 - Practical Aspects of Deep Learning (He Initialization, L2 Regularization & Dropout, GC)  

+ ### Week 2 - Optimization Algorithms (Mini-batch, Momentum & RMSprop & Adam, Learning Rate Decay, Local Optima and Saddle Points)

+ ### Week 3 - Hyperparameter Tuning, Batch Normalization and Programming Frameworks

## About

Course link: https://www.coursera.org/learn/deep-neural-network

This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. 

After 3 weeks, you will: 
- Understand industry best-practices for building deep learning applications. 
- Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

This is the second course of the Deep Learning Specialization.